import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, ConfusionMatrixDisplay

# ======================
# Funci√≥n para cargar datos
# ======================
@st.cache_data
def load_default_data():
    X, y = make_classification(
        n_samples=300, n_features=6, n_informative=4,
        n_redundant=0, n_classes=3, random_state=42
    )
    df = pd.DataFrame(X, columns=[f"Feature_{i}" for i in range(X.shape[1])])
    df["Target"] = y
    return df

# ======================
# App principal
# ======================
st.title("üìä Clasificaci√≥n con Machine Learning")
st.write("KNN | √Årbol de Decisi√≥n | Naive Bayes")

# Opci√≥n de carga de datos
uploaded_file = st.file_uploader("üìÇ Carga tu propio CSV", type=["csv"])
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.success("‚úÖ CSV cargado con √©xito")
else:
    df = load_default_data()
    st.info("‚ö†Ô∏è No subiste un archivo. Se est√° usando un dataset simulado.")

# Mostrar preview
st.subheader("üîé Vista previa de los datos")
st.dataframe(df.head())

# ======================
# EDA
# ======================
st.subheader("üìà An√°lisis Exploratorio de Datos (EDA)")

if st.checkbox("Mostrar descripci√≥n estad√≠stica"):
    st.write(df.describe())

if st.checkbox("Mostrar correlaci√≥n entre variables"):
    fig, ax = plt.subplots(figsize=(6, 4))
    sns.heatmap(df.corr(), annot=True, cmap="coolwarm", ax=ax)
    st.pyplot(fig)

if st.checkbox("Mostrar histogramas"):
    df.hist(figsize=(10, 6))
    st.pyplot(plt.gcf())

if st.checkbox("Mostrar pairplot"):
    st.write("‚ö†Ô∏è Esto puede tardar con muchos datos")
    fig = sns.pairplot(df, hue="Target")
    st.pyplot(fig)

# ======================
# Selecci√≥n de variables
# ======================
features = st.multiselect("Selecciona las variables predictoras", df.columns[:-1], default=df.columns[:-1].tolist())
target = st.selectbox("Selecciona la variable objetivo", df.columns, index=len(df.columns)-1)

X = df[features]
y = df[target]

# ======================
# Divisi√≥n de datos
# ======================
test_size = st.slider("Proporci√≥n de test", 0.1, 0.5, 0.3)
random_state = st.number_input("Random state", value=42, step=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

# ======================
# Selecci√≥n de modelo
# ======================
st.subheader("‚öôÔ∏è Modelado")
model_name = st.selectbox("Elige un modelo", ["KNN", "√Årbol de decisi√≥n", "Naive Bayes"])

if model_name == "KNN":
    n_neighbors = st.slider("N√∫mero de vecinos (K)", 1, 15, 5)
    model = KNeighborsClassifier(n_neighbors=n_neighbors)

elif model_name == "√Årbol de decisi√≥n":
    max_depth = st.slider("Profundidad m√°xima", 1, 10, 3)
    criterion = st.selectbox("Criterio", ["gini", "entropy"])
    model = DecisionTreeClassifier(max_depth=max_depth, criterion=criterion, random_state=random_state)

else:  # Naive Bayes
    model = GaussianNB()

# Entrenar modelo
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# ======================
# Resultados
# ======================
st.subheader("üìä Resultados del modelo")
st.text(classification_report(y_test, y_pred))

# Matriz de confusi√≥n
fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, ax=ax, cmap="Blues")
st.pyplot(fig)

# ======================
# Visualizaci√≥n de fronteras de decisi√≥n
# ======================
if len(features) == 2:
    st.subheader("üåê Fronteras de decisi√≥n")
    x_min, x_max = X[features[0]].min() - 1, X[features[0]].max() + 1
    y_min, y_max = X[features[1]].min() - 1, X[features[1]].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    fig, ax = plt.subplots()
    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
    scatter = ax.scatter(X_test[features[0]], X_test[features[1]], c=y_test, edgecolor="k", cmap=plt.cm.coolwarm)
    ax.set_xlabel(features[0])
    ax.set_ylabel(features[1])
    st.pyplot(fig)
else:
    st.info("‚ö†Ô∏è Para ver fronteras de decisi√≥n selecciona exactamente 2 variables predictoras.")

# ======================
# Visualizaci√≥n del √°rbol
# ======================
if model_name == "√Årbol de decisi√≥n":
    st.subheader("üå≥ Visualizaci√≥n del √Årbol de Decisi√≥n")
    fig, ax = plt.subplots(figsize=(12, 6))
    plot_tree(model, filled=True, feature_names=features, class_names=[str(c) for c in np.unique(y)], ax=ax)
    st.pyplot(fig)
